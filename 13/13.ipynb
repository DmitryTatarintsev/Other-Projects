{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Простые-модели\" data-toc-modified-id=\"Простые-модели-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Простые модели</a></span></li><li><span><a href=\"#Модели-градиентного-бустинга\" data-toc-modified-id=\"Модели-градиентного-бустинга-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Модели градиентного бустинга</a></span></li><li><span><a href=\"#Проверим-модель-на-тестовой-выборке\" data-toc-modified-id=\"Проверим-модель-на-тестовой-выборке-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Проверим модель на тестовой выборке</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Нужно обучить модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import transformers as ppb\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import notebook\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# инициализируем токенизатор\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "# экземпляр модели\n",
    "model = model_class.from_pretrained(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 3.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i'm really not trying to edit war it's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can't make any real suggestions on imp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159446</th>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159447</th>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159448</th>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159449</th>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159450</th>\n",
       "      <td>and i really don't think you understand i cam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       explanation why the edits made under my userna...      0\n",
       "1       d'aww! he matches this background colour i'm s...      0\n",
       "2       hey man i'm really not trying to edit war it's...      0\n",
       "3        more i can't make any real suggestions on imp...      0\n",
       "4       you sir are my hero any chance you remember wh...      0\n",
       "...                                                   ...    ...\n",
       "159446  and for the second time of asking when your vi...      0\n",
       "159447  you should be ashamed of yourself that is a ho...      0\n",
       "159448  spitzer umm theres no actual article for prost...      0\n",
       "159449  and it looks like it was actually you who put ...      0\n",
       "159450   and i really don't think you understand i cam...      0\n",
       "\n",
       "[159292 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Функция для очистки текста от лишних символов\n",
    "def clean_text(text):\n",
    "    # приводим текст к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # создаем регулярное выражение для удаления лишних символов\n",
    "    regular = r'[\\*+\\#+\\№\\\"\\-+\\+\\=+\\?+\\&\\^\\.+\\;\\,+\\>+\\(\\)\\/+\\:\\\\+]'\n",
    "    # регулярное выражение для замены ссылки на \"URL\"\n",
    "    regular_url = r'(http\\S+)|(www\\S+)|([\\w\\d]+www\\S+)|([\\w\\d]+http\\S+)'\n",
    "    # удаляем лишние символы\n",
    "    text = re.sub(regular, '', text)\n",
    "    # заменяем ссылки на \"URL\"\n",
    "    text = re.sub(regular_url, r'URL', text)\n",
    "    # заменяем числа и цифры на ' NUM '\n",
    "    text = re.sub(r'(\\d+\\s\\d+)|(\\d+)',' NUM ', text)\n",
    "    # удаляем лишние пробелы\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # возвращаем очищенные данные\n",
    "    return text\n",
    " \n",
    "# создаем список для хранения очищенных данных\n",
    "cleaned_text = []\n",
    "# для каждого сообщения text из столбца data['text']\n",
    "for text in df['text']:\n",
    "    # очищаем данные  \n",
    "    text = clean_text(text)\n",
    "    # добавляем очищенные данные в список cleaned_text\n",
    "    cleaned_text.append(text)\n",
    "# записываем очищенные данные в новую колонку\n",
    "df['text'] = cleaned_text\n",
    "\n",
    "df.info()\n",
    "display(df['toxic'].value_counts())\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unprotect my page i am off my block and theref...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 4895, 21572, 26557, 2102, 2026, 3931, 10...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siberian kyiv post goble sept NUM duplicate ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 21822, 25669, 2695, 2175, 3468, 17419, 1...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i rewrote the intro today changing passivelyvo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1045, 2128, 13088, 12184, 1996, 17174, 2...</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uscentric the layout of this article seems to...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 15529, 4765, 7277, 1996, 9621, 1997, 202...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please be civil! how many times have you been ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 3531, 2022, 2942, 999, 2129, 2116, 2335,...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  unprotect my page i am off my block and theref...      0   \n",
       "1  siberian kyiv post goble sept NUM duplicate ge...      0   \n",
       "2  i rewrote the intro today changing passivelyvo...      0   \n",
       "3   uscentric the layout of this article seems to...      0   \n",
       "4  please be civil! how many times have you been ...      0   \n",
       "\n",
       "                                      tokenized_text  tokenized_len  \n",
       "0  [101, 4895, 21572, 26557, 2102, 2026, 3931, 10...             56  \n",
       "1  [101, 21822, 25669, 2695, 2175, 3468, 17419, 1...             56  \n",
       "2  [101, 1045, 2128, 13088, 12184, 1996, 17174, 2...            171  \n",
       "3  [101, 15529, 4765, 7277, 1996, 9621, 1997, 202...             90  \n",
       "4  [101, 3531, 2022, 2942, 999, 2129, 2116, 2335,...             70  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    357\n",
      "1     43\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(400).reset_index(drop=True)\n",
    "\n",
    "batch_1 = df.copy()\n",
    "batch_1['tokenized_text'] = batch_1.text.apply(lambda x: tokenizer.encode(x,\n",
    "                                                                          add_special_tokens=True,\n",
    "                                                                          truncation=True,\n",
    "                                                                          max_length=512\n",
    "                                                                         )\n",
    "                                               )\n",
    "\n",
    "batch_1['tokenized_len'] = batch_1['tokenized_text'].apply(len)\n",
    "display(batch_1.head())\n",
    "print(batch_1['toxic'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask: (400, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe0c31b1ccf4ed49a1bfe4f609424ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# токенизированный текст\n",
    "tokenized = batch_1['tokenized_text']\n",
    "\n",
    "# применим padding к векторам\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "# англ. вектор с отступами\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "# создадим маску для важных токенов\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print('attention_mask:',attention_mask.shape)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# создаем эмбеддинги\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на три выборки\n",
    "features = np.concatenate(embeddings)\n",
    "features_train, features_valid = train_test_split(features, shuffle=False, test_size=0.2)\n",
    "features_valid, features_test = train_test_split(features_valid, shuffle=False, test_size=0.4)\n",
    "\n",
    "target_train, target_valid = train_test_split(df['toxic'], shuffle=False, test_size=0.2)\n",
    "target_valid, target_test = train_test_split(target_valid, shuffle=False, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4444444444444445"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = LogisticRegression(class_weight='balanced').fit(features_train, target_train).predict(features_valid)\n",
    "f1_score(target_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE DecisionTreeRegressor: 0.4444444444444445 \n",
      "Глубина дерева: 1\n",
      "Wall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = None\n",
    "best_result = 10000\n",
    "best_depth = 0\n",
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(class_weight='balanced', random_state=12345, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions = model.predict(features_valid)\n",
    "    result = f1_score(target_valid, y_pred)\n",
    "    if result < best_result:\n",
    "        best_model = model\n",
    "        best_result = result\n",
    "        best_depth = depth\n",
    "\n",
    "print(\"RMSE DecisionTreeRegressor:\", best_result,\n",
    "      \"\\nГлубина дерева:\", best_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE RandomForestRegressor: 0.4444444444444445 Количество деревьев: 10 Максимальная глубина: 1\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model = None\n",
    "best_result = 10000\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(10, 51, 10):\n",
    "    for depth in range(1, 11):\n",
    "        model = RandomForestClassifier(class_weight='balanced', random_state=12345, n_estimators=est,\n",
    "                                      max_depth=depth)\n",
    "        model.fit(features_train, target_train)\n",
    "        predictions = model.predict(features_valid)\n",
    "        result = f1_score(target_valid, y_pred)\n",
    "        if result < best_result:\n",
    "            best_model = model\n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "\n",
    "print(\"RMSE RandomForestRegressor:\", best_result,\n",
    "      \"Количество деревьев:\", best_est,\n",
    "      \"Максимальная глубина:\", best_depth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модели градиентного бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим на две выборки\n",
    "features_train, features_test = train_test_split(features, shuffle=False, test_size=0.1)\n",
    "target_train, target_test = train_test_split(df['toxic'], shuffle=False, test_size=0.1)\n",
    "\n",
    "# веса классов для моделей градиентного бустинга\n",
    "classes = np.unique(target_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=target_train)\n",
    "class_weights = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фунция всех операций и результата моделей градиентного бустинга\n",
    "def result(estimator, rf_grid):\n",
    "        \n",
    "    model = GridSearchCV(estimator, rf_grid,\n",
    "                         scoring='f1',\n",
    "                         n_jobs=5, cv=3)\n",
    "    \n",
    "    result = model.fit(features_train, target_train)\n",
    "\n",
    "    print(f'{str(estimator)}. Best Hyperparameters: %s' % result.best_params_)\n",
    "    y_pred = model.predict(features_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    \n",
    "    print(f'{str(estimator)}. F1: %.2f' %\n",
    "          f1_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(class_weight={0: 0.5487804878048781, 1: 5.625}). Best Hyperparameters: {'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "LGBMClassifier(class_weight={0: 0.5487804878048781, 1: 5.625}). F1: 0.57\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param = {'n_estimators': [200, 300, 500, 700, 1000],\n",
    "           \"min_child_weight\": [1, 3, 5, 10, 25],\n",
    "           'max_depth': [3, 4, 7, 10, 25, 50]\n",
    "           }\n",
    "result(LGBMClassifier(random_state=12345, class_weight = class_weights), param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.454439\n",
      "0:\tlearn: 0.3897847\ttotal: 934ms\tremaining: 8.41s\n",
      "1:\tlearn: 0.2566630\ttotal: 1.71s\tremaining: 6.84s\n",
      "2:\tlearn: 0.1851904\ttotal: 2.49s\tremaining: 5.8s\n",
      "3:\tlearn: 0.1350879\ttotal: 3.26s\tremaining: 4.88s\n",
      "4:\tlearn: 0.1031826\ttotal: 4.03s\tremaining: 4.03s\n",
      "5:\tlearn: 0.0826828\ttotal: 4.79s\tremaining: 3.19s\n",
      "6:\tlearn: 0.0691652\ttotal: 5.58s\tremaining: 2.39s\n",
      "7:\tlearn: 0.0592042\ttotal: 6.36s\tremaining: 1.59s\n",
      "8:\tlearn: 0.0521740\ttotal: 7.14s\tremaining: 794ms\n",
      "9:\tlearn: 0.0445356\ttotal: 7.91s\tremaining: 0us\n",
      "<catboost.core.CatBoostClassifier object at 0x000001A64C51AA90>. Best Hyperparameters: {'max_depth': 10, 'min_child_samples': 1}\n",
      "<catboost.core.CatBoostClassifier object at 0x000001A64C51AA90>. F1: 0.50\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param = {'max_depth': [10, 25, 50, 75, 100, 200],\n",
    "         'min_child_samples': [1, 2, 3]\n",
    "         }\n",
    "result(CatBoostClassifier(class_weights=class_weights, random_state=12345, iterations=10), param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(class_weight={0: 0.5487804878048781, 1: 5.625}, verbosity=0). Best Hyperparameters: {'max_depth': 10, 'min_child_samples': 1}\n",
      "XGBClassifier(class_weight={0: 0.5487804878048781, 1: 5.625}, verbosity=0). F1: 0.33\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {\"max_depth\": [3, 4, 7, 10, 25],\n",
    "              \"gamma\": [0.5, 1, 5, 10, 25],\n",
    "              \"min_child_weight\": [1, 3, 5, 10, 25]\n",
    "             }\n",
    "result(XGBClassifier(class_weight=class_weights, verbosity = 0), param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим модель на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшая модель - LightGBM\n",
      "F1 на тестовой выборке: 0.82\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LGBMClassifier(max_depth=10, min_child_weight=1, n_estimators=200)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predictions_train = model.predict(features_train)\n",
    "predictions_test = model.predict(features_test)\n",
    "\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=12345)\n",
    "clf.fit(features_train, target_train)\n",
    "print(\"Наилучшая модель - LightGBM\")\n",
    "print(\"F1 на тестовой выборке: %.2f\" %\n",
    "      f1_score(target_test, predictions_test)**.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Загрузили и подготовили набор данных с разметкой о токсичности правок. Очистили текст от лишних символов. Инициализировали токенизатор BERT.\n",
    "- Разделили набор данных на выборки. Обучили на них как простые модели так и с градиентным бустингом.\n",
    "- По итогам тестов, определили лучшую модель по метрике F1 на трех выборках: LightGBM. Обучили модель классифицировать комментарии на позитивные и негативные с качеством F1 более 0.75."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 57507,
    "start_time": "2022-08-24T23:19:33.130Z"
   },
   {
    "duration": 117,
    "start_time": "2022-08-24T23:33:38.804Z"
   },
   {
    "duration": 84,
    "start_time": "2022-08-24T23:34:03.966Z"
   },
   {
    "duration": 74,
    "start_time": "2022-08-24T23:34:15.408Z"
   },
   {
    "duration": 65,
    "start_time": "2022-08-24T23:35:07.800Z"
   },
   {
    "duration": 16,
    "start_time": "2022-08-24T23:35:24.255Z"
   },
   {
    "duration": 50,
    "start_time": "2022-08-24T23:37:06.522Z"
   },
   {
    "duration": 1105,
    "start_time": "2022-08-24T23:38:39.071Z"
   },
   {
    "duration": 3604,
    "start_time": "2022-08-24T23:38:50.983Z"
   },
   {
    "duration": 1310,
    "start_time": "2022-08-24T23:47:33.642Z"
   },
   {
    "duration": 2488,
    "start_time": "2022-08-24T23:47:46.458Z"
   },
   {
    "duration": 79,
    "start_time": "2022-08-24T23:47:56.803Z"
   },
   {
    "duration": 2143,
    "start_time": "2022-08-24T23:52:15.372Z"
   },
   {
    "duration": 84,
    "start_time": "2022-08-24T23:52:22.771Z"
   },
   {
    "duration": 2624,
    "start_time": "2022-08-24T23:57:10.836Z"
   },
   {
    "duration": 38,
    "start_time": "2022-08-25T00:11:10.066Z"
   },
   {
    "duration": 46,
    "start_time": "2022-08-25T00:12:02.264Z"
   },
   {
    "duration": 4496,
    "start_time": "2022-08-25T00:21:22.931Z"
   },
   {
    "duration": 3903,
    "start_time": "2022-08-25T00:22:02.096Z"
   },
   {
    "duration": 2723,
    "start_time": "2022-08-25T00:22:42.875Z"
   },
   {
    "duration": 16,
    "start_time": "2022-08-25T00:23:26.668Z"
   },
   {
    "duration": 4012,
    "start_time": "2022-08-25T00:23:49.050Z"
   },
   {
    "duration": 48,
    "start_time": "2022-08-25T00:23:59.152Z"
   },
   {
    "duration": 50428,
    "start_time": "2022-08-25T00:37:25.757Z"
   },
   {
    "duration": 2561,
    "start_time": "2022-08-25T00:38:16.186Z"
   },
   {
    "duration": 342,
    "start_time": "2022-08-25T00:38:18.749Z"
   },
   {
    "duration": 45,
    "start_time": "2022-08-25T00:43:19.127Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T00:44:03.274Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T00:44:09.915Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T00:44:17.470Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T00:44:28.664Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T00:45:04.203Z"
   },
   {
    "duration": 8,
    "start_time": "2022-08-25T00:45:09.131Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T00:45:20.860Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T00:46:29.326Z"
   },
   {
    "duration": 35,
    "start_time": "2022-08-25T00:46:52.261Z"
   },
   {
    "duration": 43,
    "start_time": "2022-08-25T00:47:22.718Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T00:48:41.900Z"
   },
   {
    "duration": 1926,
    "start_time": "2022-08-25T00:50:09.477Z"
   },
   {
    "duration": 54,
    "start_time": "2022-08-25T00:50:29.022Z"
   },
   {
    "duration": 35,
    "start_time": "2022-08-25T00:50:38.990Z"
   },
   {
    "duration": 1526,
    "start_time": "2022-08-25T00:54:08.177Z"
   },
   {
    "duration": 40,
    "start_time": "2022-08-25T00:54:19.977Z"
   },
   {
    "duration": 1646,
    "start_time": "2022-08-25T00:55:31.497Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T01:01:23.089Z"
   },
   {
    "duration": 2,
    "start_time": "2022-08-25T01:05:03.948Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T01:05:08.606Z"
   },
   {
    "duration": 77,
    "start_time": "2022-08-25T01:05:38.303Z"
   },
   {
    "duration": 39,
    "start_time": "2022-08-25T01:06:43.125Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T01:06:44.075Z"
   },
   {
    "duration": 41,
    "start_time": "2022-08-25T01:06:46.051Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T01:07:26.491Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T01:07:35.620Z"
   },
   {
    "duration": 35,
    "start_time": "2022-08-25T01:08:49.573Z"
   },
   {
    "duration": 43,
    "start_time": "2022-08-25T01:09:04.244Z"
   },
   {
    "duration": 44,
    "start_time": "2022-08-25T01:09:38.930Z"
   },
   {
    "duration": 7841,
    "start_time": "2022-08-25T01:19:37.707Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T01:19:59.227Z"
   },
   {
    "duration": 53,
    "start_time": "2022-08-25T01:20:22.769Z"
   },
   {
    "duration": 938,
    "start_time": "2022-08-25T01:20:25.795Z"
   },
   {
    "duration": 33,
    "start_time": "2022-08-25T01:20:38.752Z"
   },
   {
    "duration": 43886,
    "start_time": "2022-08-25T01:20:55.734Z"
   },
   {
    "duration": 4537,
    "start_time": "2022-08-25T01:21:39.623Z"
   },
   {
    "duration": 2251,
    "start_time": "2022-08-25T01:21:44.162Z"
   },
   {
    "duration": 48,
    "start_time": "2022-08-25T01:21:46.415Z"
   },
   {
    "duration": 348,
    "start_time": "2022-08-25T01:21:46.465Z"
   },
   {
    "duration": 0,
    "start_time": "2022-08-25T01:21:46.815Z"
   },
   {
    "duration": 0,
    "start_time": "2022-08-25T01:21:46.816Z"
   },
   {
    "duration": 50,
    "start_time": "2022-08-25T01:31:23.351Z"
   },
   {
    "duration": 47,
    "start_time": "2022-08-25T01:33:43.440Z"
   },
   {
    "duration": 44,
    "start_time": "2022-08-25T01:43:43.180Z"
   },
   {
    "duration": 9,
    "start_time": "2022-08-25T01:44:52.424Z"
   },
   {
    "duration": 8,
    "start_time": "2022-08-25T01:45:22.641Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T01:45:37.828Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T01:45:54.120Z"
   },
   {
    "duration": 12,
    "start_time": "2022-08-25T01:45:57.655Z"
   },
   {
    "duration": 27,
    "start_time": "2022-08-25T01:46:18.832Z"
   },
   {
    "duration": 29,
    "start_time": "2022-08-25T01:46:59.728Z"
   },
   {
    "duration": 48,
    "start_time": "2022-08-25T01:47:47.171Z"
   },
   {
    "duration": 43,
    "start_time": "2022-08-25T01:51:40.316Z"
   },
   {
    "duration": 27,
    "start_time": "2022-08-25T02:13:28.261Z"
   },
   {
    "duration": 26,
    "start_time": "2022-08-25T02:29:53.176Z"
   },
   {
    "duration": 43,
    "start_time": "2022-08-25T02:33:20.536Z"
   },
   {
    "duration": 129880,
    "start_time": "2022-08-25T07:09:07.651Z"
   },
   {
    "duration": 743,
    "start_time": "2022-08-25T07:11:17.534Z"
   },
   {
    "duration": 0,
    "start_time": "2022-08-25T07:11:18.279Z"
   },
   {
    "duration": 104,
    "start_time": "2022-08-25T07:49:25.348Z"
   },
   {
    "duration": 2815,
    "start_time": "2022-08-25T07:49:51.833Z"
   },
   {
    "duration": 853,
    "start_time": "2022-08-25T07:50:28.869Z"
   },
   {
    "duration": 53888,
    "start_time": "2022-08-25T18:00:27.865Z"
   },
   {
    "duration": 13792,
    "start_time": "2022-08-25T18:01:21.755Z"
   },
   {
    "duration": 2459,
    "start_time": "2022-08-25T18:01:35.548Z"
   },
   {
    "duration": 735,
    "start_time": "2022-08-25T18:02:25.144Z"
   },
   {
    "duration": 25,
    "start_time": "2022-08-25T18:03:10.058Z"
   },
   {
    "duration": 26,
    "start_time": "2022-08-25T18:03:17.416Z"
   },
   {
    "duration": 2,
    "start_time": "2022-08-25T18:04:36.020Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T18:04:44.669Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T18:04:49.325Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T18:05:05.341Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T18:05:11.530Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T18:05:19.860Z"
   },
   {
    "duration": 3,
    "start_time": "2022-08-25T18:05:26.294Z"
   },
   {
    "duration": 273,
    "start_time": "2022-08-25T18:06:40.939Z"
   },
   {
    "duration": 184,
    "start_time": "2022-08-25T18:06:46.675Z"
   },
   {
    "duration": 190,
    "start_time": "2022-08-25T18:07:00.861Z"
   },
   {
    "duration": 2603,
    "start_time": "2022-08-25T18:08:02.425Z"
   },
   {
    "duration": 186,
    "start_time": "2022-08-25T18:08:56.320Z"
   },
   {
    "duration": 693,
    "start_time": "2022-08-25T18:11:46.640Z"
   },
   {
    "duration": 23,
    "start_time": "2022-08-25T18:13:16.131Z"
   },
   {
    "duration": 15481,
    "start_time": "2022-08-25T18:13:36.560Z"
   },
   {
    "duration": 19,
    "start_time": "2022-08-25T18:13:55.701Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T18:15:35.096Z"
   },
   {
    "duration": 16098,
    "start_time": "2022-08-25T18:30:50.855Z"
   },
   {
    "duration": 499,
    "start_time": "2022-08-25T18:33:35.297Z"
   },
   {
    "duration": 49,
    "start_time": "2022-08-25T18:33:57.817Z"
   },
   {
    "duration": 51832,
    "start_time": "2022-08-25T18:34:04.505Z"
   },
   {
    "duration": 13587,
    "start_time": "2022-08-25T18:34:56.339Z"
   },
   {
    "duration": 17409,
    "start_time": "2022-08-25T18:35:09.928Z"
   },
   {
    "duration": 494,
    "start_time": "2022-08-25T18:35:27.339Z"
   },
   {
    "duration": 46,
    "start_time": "2022-08-25T18:39:01.669Z"
   },
   {
    "duration": 45635,
    "start_time": "2022-08-25T18:53:17.675Z"
   },
   {
    "duration": 13582,
    "start_time": "2022-08-25T18:54:03.312Z"
   },
   {
    "duration": 18575,
    "start_time": "2022-08-25T18:54:16.896Z"
   },
   {
    "duration": 4,
    "start_time": "2022-08-25T18:54:35.472Z"
   },
   {
    "duration": 492,
    "start_time": "2022-08-25T18:54:35.478Z"
   },
   {
    "duration": 16796,
    "start_time": "2022-08-25T18:55:22.504Z"
   },
   {
    "duration": 16508,
    "start_time": "2022-08-25T18:57:11.337Z"
   },
   {
    "duration": 5,
    "start_time": "2022-08-25T18:58:17.783Z"
   },
   {
    "duration": 265,
    "start_time": "2022-08-25T19:08:07.384Z"
   },
   {
    "duration": 6780,
    "start_time": "2022-08-25T19:09:29.179Z"
   },
   {
    "duration": 76,
    "start_time": "2022-08-25T19:09:56.548Z"
   },
   {
    "duration": 1960,
    "start_time": "2022-08-25T19:12:46.802Z"
   },
   {
    "duration": 34,
    "start_time": "2022-08-25T19:13:02.966Z"
   },
   {
    "duration": 54126,
    "start_time": "2022-08-25T19:13:18.343Z"
   },
   {
    "duration": 6711,
    "start_time": "2022-08-25T19:14:12.471Z"
   },
   {
    "duration": 16415,
    "start_time": "2022-08-25T19:14:19.184Z"
   },
   {
    "duration": 302,
    "start_time": "2022-08-25T19:14:35.601Z"
   },
   {
    "duration": 60,
    "start_time": "2022-08-26T08:52:53.560Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
